[{"content":" ü§ñ UR5e Cube Grasp Robotic Manipulation with Visual Feedback\nüìã Project Overview This project demonstrates autonomous robotic manipulation using the UR5e robotic arm combined with an Intel RealSense D435 depth camera for visual feedback. The system can identify, locate, and grasp colored cubes placed on a tabletop.\nüéØ Accuracy ¬±2mm positioning ‚ö° Speed 3 sec per grasp ‚úÖ Success Rate 95% grasp success üë• Team Members A Alice Chen Perception Lead B Bob Wang Controls Lead C Carol Zhang Integration Lead üîß Technical Approach Perception Pipeline The system uses a 3-stage perception pipeline:\nColor Segmentation ‚Äî HSV-based color filtering to isolate cube pixels Depth Estimation ‚Äî Point cloud processing to determine 3D position Pose Estimation ‚Äî PCA-based orientation detection for grasp planning def detect_cube(rgb_image, depth_image): # Convert to HSV and create mask hsv = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2HSV) mask = cv2.inRange(hsv, lower_bound, upper_bound) # Find centroid moments = cv2.moments(mask) cx = int(moments[\u0026#39;m10\u0026#39;] / moments[\u0026#39;m00\u0026#39;]) cy = int(moments[\u0026#39;m01\u0026#39;] / moments[\u0026#39;m00\u0026#39;]) # Get 3D position from depth depth = depth_image[cy, cx] point_3d = camera_to_world(cx, cy, depth) return point_3d Motion Planning We implemented RRT-Connect for collision-free path planning and used MoveIt2 for trajectory execution. The gripper approach is computed using the cube\u0026rsquo;s estimated pose to ensure proper alignment.\nüí° Key Insight Using a pre-grasp pose 5cm above the target significantly improved grasp success by allowing the controller to correct for small positioning errors during the final approach. üìä Results Our system achieved the following performance metrics:\nMetric Value Grasp Success Rate 95.2% Average Cycle Time 8.3 seconds Position Accuracy ¬±2.1 mm Orientation Error ¬±3.5¬∞ üé¨ Demo Video üîó Resources üìÅ GitHub Repository üìÑ Project Report üéûÔ∏è Presentation Slides ","permalink":"https://dongc1.github.io/cosmos-eecs106a.github.io/posts/hello-world/","summary":"\u003cdiv style=\"\n  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n  padding: 60px 40px;\n  border-radius: 16px;\n  margin-bottom: 40px;\n  text-align: center;\n  color: white;\n\"\u003e\n  \u003cspan style=\"font-size: 5rem; display: block; margin-bottom: 16px;\"\u003eü§ñ\u003c/span\u003e\n  \u003ch1 style=\"font-size: 2.5rem; font-weight: 800; margin: 0; color: white;\"\u003eUR5e Cube Grasp\u003c/h1\u003e\n  \u003cp style=\"font-size: 1.125rem; opacity: 0.9; margin-top: 12px; color: white;\"\u003eRobotic Manipulation with Visual Feedback\u003c/p\u003e\n\u003c/div\u003e\n\u003ch2 id=\"-project-overview\"\u003eüìã Project Overview\u003c/h2\u003e\n\u003cp\u003eThis project demonstrates autonomous robotic manipulation using the \u003cstrong\u003eUR5e robotic arm\u003c/strong\u003e combined with an \u003cstrong\u003eIntel RealSense D435\u003c/strong\u003e depth camera for visual feedback. The system can identify, locate, and grasp colored cubes placed on a tabletop.\u003c/p\u003e","title":"UR5e Cube Grasp Project"},{"content":" Meet Our TeamEECS 106/206A Final Project Team\nGeorge MaPhD Student Yiren RongMaster Student Jiayi ZhuUndergrad Student DDi TianMaster Student MMingliang TangPhD Student ","permalink":"https://dongc1.github.io/cosmos-eecs106a.github.io/about/","summary":"\u003cstyle\u003e\n.about-hero { width: 100vw; position: relative; left: 50%; right: 50%; margin-left: -50vw; margin-right: -50vw; margin-top: -30px; padding-top: 200px; padding-bottom: 100px; padding-left: 40px; padding-right: 40px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); text-align: center; color: white; margin-bottom: 80px; }\n.about-hero h1 { font-size: 2.5rem; font-weight: 800; margin: 0 0 16px 0; color: white; }\n.about-hero p { font-size: 1.125rem; opacity: 0.95; max-width: 600px; margin: 0 auto; color: white; }\n.team-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 32px; max-width: 1100px; margin: 0 auto; }\n.team-card { background: white; border: 1px solid #E4E6EB; border-radius: 16px; padding: 32px; text-align: center; transition: all 0.3s ease; box-shadow: 0 2px 8px rgba(0,0,0,0.04); }\n.team-card:hover { transform: translateY(-8px); box-shadow: 0 12px 40px rgba(0,0,0,0.12); }\n.team-avatar { width: 120px; height: 120px; border-radius: 50%; margin: 0 auto 20px; display: flex; align-items: center; justify-content: center; color: white; font-size: 2.5rem; font-weight: 700; overflow: hidden; }\n.team-avatar img { width: 100%; height: 100%; object-fit: cover; }\n.team-name { font-size: 1.25rem; font-weight: 700; color: #1C1E21; margin-bottom: 8px; }\n.team-role { display: inline-block; background: #E7F3FF; color: #0668E1; padding: 6px 16px; border-radius: 20px; font-size: 0.8125rem; font-weight: 600; }\n@media (max-width: 768px) {\n  .about-hero { padding: 80px 24px; }\n  .about-hero h1 { font-size: 2rem; }\n  .team-grid { grid-template-columns: repeat(auto-fit, minmax(160px, 1fr)); gap: 24px; }\n  .team-avatar { width: 100px; height: 100px; font-size: 2rem; }\n}\n\u003c/style\u003e\n\u003cdiv class=\"about-hero\"\u003e\u003ch1\u003eMeet Our Team\u003c/h1\u003e\u003cp\u003eEECS 106/206A Final Project Team\u003c/p\u003e","title":""}]